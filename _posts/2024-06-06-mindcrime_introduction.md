---
layout: post
title: "Ментальное преступление: Введение"
translated_by: К. Кирдан
excerpt: "Рассмотрим машинный интеллект, который строит и тестирует лучшие модели поведения человека, какие только может. Если модель, которая дает наилучшие предсказания, включает в себя симуляции (со средней степенью изоморфности) человеческой когниции, то эта модель, по мере ее работы, сама может быть самосознающей, или сознательной, или разумной, или обладать любым другим свойством, которое делает ее объектом этической значимости. Это не значит, что работающая модель Фреда — это Фред, или того, что работающая модель Фреда — вообще человек. Но проблема заключается в том, что достаточно совершенной моделью личности будет личность, даже если они могут и не быть одним и тем же лицом."
original: https://arbital.com/p/mindcrime_introduction
license: https://creativecommons.org/licenses/by/3.0/deed.ru
---
Чем большей точности прогнозирования мы хотим от модели, тем более детализированной она становится. Очень грубая модель самолета может содержать лишь приблизительную форму, мощность двигателей и массу самолета. Но подходящая для инженерных работ модель должна быть достаточно детальной, чтобы симулировать поток воздуха над крыльями, действие центростремительной силы на лопасти вентилятора, и др. По мере того как модель может предсказывать самолёт все более и более детально, со все лучшими и лучшими распределениями вероятностей, проводимые ей для прогнозирования вычисления могут начинать выглядеть всё более и более похожими на детальную симуляцию полёта самолёта.

Рассмотрим машинный интеллект, который строит и тестирует лучшие модели поведения человека, какие только может. Если модель, которая делает _наилучшие_ предсказания, включает в себя симуляции (со средней степенью изоморфности) человеческой когниции, то эта модель, по мере ее работы, сама может быть самосознающей, или сознательной, или разумной, или обладать любым другим свойством, которое делает ее объектом этической значимости. Это не значит, что работающая модель Фреда — это Фред, или даже того, что работающая модель Фреда — вообще человек. Но проблема заключается в том, что достаточно совершенной моделью личности _будет_ личность, даже если они могут и не быть _одним и тем же_ лицом.

Поэтому мы можем быть обеспокоены тем, что, к примеру, если Фред несчастен, или _может_ быть несчастен, то агент будет рассматривать тысячи или миллионы гипотез о разных версиях Фреда. И гипотезы о страдающих версиях Фреда по мере их воспроизведения сами могут страдать. В качестве схожей проблемы, эти гипотезы о Фреде могут затем быть отброшены — прекратить работу — когда агент найдет новые свидетельства и обновит свою модель. Поскольку [программы могут быть людьми][some_computations_are_people], остановка и удаление сознательной программы — это преступление убийства.

Этот сценарий, который можно назвать «‎проблемой разумных моделей», является вариантом более общей проблемы того, что Бостром называет «‎ментальным преступлением». ([Элиезер Юдковский][EliezerYudkowsky] предложил термин «‎ментальный геноцид» как вариант с менее оруэлловской коннотацией.) В более общем случае мы можем опасаться того, что существуют агентские системы, которые наносят огромный моральный вред просто за счет того, какие вычисления они производят — потому что они содержат внутри сознательные страдания и смерть.

Другой сценарий можно назвать «проблемой разумных подсистем». Возможно, например, что наиболее эффективной системой для выделения памяти для подпроцессов, является управляющий памятью субагент, который достаточно рефлексивен, чтобы быть независимой сознательной личностью. Эта проблема отличается от проблемы создания единого сознательного и страдающего машинного интеллекта, потому что здесь сознательный субагент может быть скрыт на более глубоком уровне устройства сверхагента, и таких субагентов может быть намного _больше_ по сравнению со всего одним страдающим сверхагентом.

Оба этих сценария представляют собой нанесение морального вреда внутри производимых агентом вычислений, независимо от его внешнего поведения. Создав сверхинтеллект, мы не можем сделать вывод, что не причинили никакого вреда, полагаясь лишь на то, что сверхинтеллект никого снаружи себя не убивает. Потому что _внутри_ него могут быть триллионы страдающих и умирающих людей. Это обстоятельство отличает ментальное преступление от почти всех прочих проблем в рамках [задачи согласования ценностей][value_alignment_problem], обычно рассматривающей внешнее поведение агентов.

Чтобы избежать ментального геноцида, было бы очень удобно точно знать, какие вычисления являются или не являются сознательными, разумными или иными объектами этической значимости. Или напрямую знать, что какой-то конкретный класс вычислений _не_ относится к объектам этической значимости.

Юдковский называет [неличностным предикатом][nonperson_predicate] любой вычислимый тест, который можно было бы безопасно использовать, чтобы определить, что то или иное вычисление точно _не_ является личностью. Этот тест требует только двух возможных ответов: «‎не личность» и «‎не знаю». Это нормально, если тест говорит «‎не знаю» для каких-то вычислений, не являющихся личностями, до тех пор, пока он говорит «‎не знаю» для _всех_ людей и никогда не говорит «‎не личность», если вычисление все же сознательно. Поскольку такой тест с определенностью говорит нам лишь о неличностности, а не об обнаружении личностности в каком-либо позитивном смысле, мы можем назвать его неличностным предикатом.

Тем не менее, цель не в том, чтобы найти хоть какой-то неличностный предикат — например, предикат, который выдает «‎известно, что не личность» лишь для пустых вычислений, ничего больше не пропуская. Цель состоит в том, чтобы имеющийся неличностный предикат пропускал какие-то мощные, полезные вычисления. Мы хотим иметь возможность построить ИИ, который не был бы личностью, позволяя ему строить подпроцессы, которые, как мы будем знать, тоже не будут личностями, а также позволяя улучшать его модели окружающих людей, используя гипотезы, которые, как мы будем знать, не будут людьми. Это означает, что неличностный предикат должен пропускать какие-то проекты ИИ, проекты когнитивных подпроцессов и модели людей, которые достаточно хороши для выполнения ИИ задач, которые мы перед ним хотим поставить.

Похоже, это может оказаться очень затруднительным по нескольким причинам:

- Есть _необычайно экстремальные_ философские разногласия и путаница насчет того, какие именно программы являются или не являются сознательными или иными объектами этической ценности. (Не будет преувеличением крикнуть: «‎никто не знает, какого черта происходит».)

- Мы не можем полностью пропустить ни один полный по Тьюрингу класс программ. Мы не можем сказать раз и навсегда, что безопасно моделировать гравитационные взаимодействия в солнечной системе, если огромные гравитационные системы могут кодировать компьютеры, которые кодируют людей.

- Проблема [ближайшей незаблокированной стратегии][nearest_unblocked] относится к любой попытке запретить [продвинутому][advanced_agent] [консеквенциалистскому агенту][consequentialist] использовать наиболее эффективные или очевидные способы моделирования людей. _Следующий_ наилучший способ моделирования людей за пределами заблокированных вариантов с необычайной вероятностью будет похож на какую-нибудь странную лазейку, которая, как окажется, кодирует разум каким-то образом, который мы ранее не представляли.

Если нет заслуживающего доверия неличностного предиката, то альтернативный способ предотвратить ментальное преступление — это рассмотреть [конструкции агентов, которые по задумке _не_ занимаются детальным моделированием людей или других умов][behaviorist] — поскольку могут быть какие-то выполнимые для них [достижения решающей значимости][pivotal], не требующие наличия согласованного агента, детально моделирующего человеческие умы.

{% include routes.html %}

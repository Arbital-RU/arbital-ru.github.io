---
layout: post
title: Ментальное преступление
translated_by: К. Кирдан
translation_details: "с небольшими сокращениями, добавлены ссылки"
excerpt: "«‎Ментальное преступление» (англ. «‎mindcrime») — предложенный Ником Бостромом термин для обозначения сценариев, в которых когнитивные процессы ИИ наносят моральный вред внутри него — например, потому что ИИ внутри себя содержит триллионы страдающих сознательных существ."
original: https://arbital.com/p/mindcrime
license: https://creativecommons.org/licenses/by/3.0/deed.ru
---
«‎Ментальное преступление» (англ. «‎mindcrime») — предложенный [Ником Бостромом][NickBostrom] термин для обозначения сценариев, в которых когнитивные процессы ИИ наносят моральный вред внутри него — например, потому что ИИ внутри себя содержит триллионы страдающих сознательных существ.

Варианты того, как это может произойти:

Проблема разумных моделей людей: естественным образом появляется, если среди наилучших предсказательных моделей окружающих людей есть такие, которые достаточно детализированы, чтобы и сами быть людьми.

Проблема разумных моделей цивилизаций: естественным образом появляется, если агент пытается моделировать, например, инопланетные цивилизации (которые, возможно, моделируют его), и его модель достаточно подробна, чтобы включать в себя сознательные симуляции инопланетян.

Проблема разумных подсистем: естественным образом появляется, если наиболее эффективное устройство каких-то когнитивных подсистем включает в себя создание субагентов, которые саморефлексивны или имеют какое-то другое свойство, ведущее к сознательности или делающее личностью.

Проблема разумных моделей себя: если ИИ сознателен или возможные будущие версии ИИ сознательны, то в ходе рассмотрения возможных самомодификаций он может запускать и останавливать большое количество сознательных моделей себя.

## Содержание
- [Проблема разумных моделей людей](#Проблема_разумных_моделей_людей)
- [Проблема разумных моделей цивилизаций](#Проблема_разумных_моделей_цивилизаций)
- [Проблема разумных подсистем](#Проблема_разумных_подсистем)
- [Проблема разумных моделей себя](#Проблема_разумных_моделей_себя)
- [Сложности](#Сложности)
  - [Масштабы потенциальной катастрофы](#Масштабы_потенциальной_катастрофы)
  - [Проблема порядка разработки](#Проблема_порядка_разработки)
  - [Странность](#Странность)
- [Неличностные предикаты](#Неличностные_предикаты)
- [Пути исследований](#Пути_исследований)

<h2 id="Проблема_разумных_моделей_людей">Проблема разумных моделей людей</h2>

[Инструментальное давление][instrumental_pressure] в сторону получения высокоточных предсказаний поведения человеческих существ (или к предсказаниям контрфактуальных решений о них, или к поиску событий, которые приводят к определенным последствиям, и т.д.) может привести к тому, что ИИ будет выполнять вычисления, которые с необычайно высокой вероятностью являются личностями.

[Нереалистичным][unbounded_analysis] примером этого была бы [индукция Соломонова][solomonoff_induction], в которой предсказания делаются средствами, включающими в себя выполнение многих возможных симуляций окружающей среды и определение того, какие из них лучше всего соответствуют реальности. Из современных алгоритмов машинного обучения фильтры частиц и алгоритмы Монте-Карло тоже включают в себя выполнение многих возможных симулированных версий системы.

Вполне возможно, что ИИ, достаточно продвинутый для составления детальных моделей человеческого интеллекта, обычно будет продвинутым настолько, чтобы никогда не использовать предсказательную/поисковую модель, которая занимается симуляциями людей [методом «грубой силы»](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%BB%D0%BD%D1%8B%D0%B9_%D0%BF%D0%B5%D1%80%D0%B5%D0%B1%D0%BE%D1%80). (К примеру, если обычно будет много возможных настроек переменных внутри модели, то эффективная модель вместо рассмотрения каких-либо точно и в целости смоделированных конкретных людей может манипулировать данными, представляющими распределение вероятностей по этим настройкам переменных.)

Однако, это не гарантирует, что никаких ментальных преступлений не будет. Для создания сознательной модели ИИ могут не требоваться точные симуляции конкретных людей. Эффективная модель (размаха возможностей) человека все равно может содержать _достаточное_ количество вычислений, _достаточно_ похожих на личность, чтобы образовать сознание или дать модели любые другие свойства, делающие ее личностью.

Почти наверняка нет необходимости спускаться до нейронного уровня, чтобы создать разумное существо. Точно также может оказаться, что хотя некоторые части разума рассматриваются лишь абстрактно, остальные вычислены достаточно детально, чтобы породить сознание, разум, личностность и т. д.

Проблему разумных моделей не следует путать с [гипотезой симуляции](https://ru.wikipedia.org/wiki/%D0%93%D0%B8%D0%BF%D0%BE%D1%82%D0%B5%D0%B7%D0%B0_%D1%81%D0%B8%D0%BC%D1%83%D0%BB%D1%8F%D1%86%D0%B8%D0%B8), т. к. эффективная модель человека не обязана иметь субъективный опыт, неотличимый от человеческого (хотя она будет моделью человека, который не верит в то, что он — лишь модель). Обсуждаемая проблема появляется, когда модель _является личностью_, а не когда она _тождественна_ тому лицу, которое она моделирует — последнее обстоятельство не играет роли в оценках последствий морального вреда.

Помимо задач, которые прямо или очевидно связаны с моделированием людей, есть много других практических задач и вопросов, решению которых может помогать моделирование других умов. Например, пониманию инструкции на тостерной печи может помочь понимание намерений ума, который пытался объяснить, как пользоваться тостером. Таким образом, ментальное преступление может быть результатом того, что достаточно мощный ИИ пытается решить совершенно обыденные задачи.

<h2 id="Проблема_разумных_моделей_цивилизаций">Проблема разумных моделей цивилизаций</h2>

Отдельный путь к ментальному преступлению исходит от продвинутого агента, который достаточно подробно рассматривает возможное происхождение и будущее разумной жизни в других мирах. (Представьте, что вам внезапно сообщили, что эта версия вас на самом деле встроена в сверхинтеллект, который представляет себе, как может развиваться жизнь в месте, подобном Земле, и что вычисление вашей жизни производило недостаточно ценной информации и его собираются остановить. Вы, вероятно, будете рассержены! Мы должны попытаться не сердить вот так других людей.)

Есть три возможных причины [конвергентного инструментального давления][instrumental_convergence] в сторону [детального рассмотрения разумных цивилизаций][distant_SIs]:

- Назначение достаточной вероятности существованию незаметных внеземных разумных существ в окрестностях Земли — возможно, в процесса рассмотрения [парадокса Ферми](https://ru.wikipedia.org/wiki/%D0%9F%D0%B0%D1%80%D0%B0%D0%B4%D0%BE%D0%BA%D1%81_%D0%A4%D0%B5%D1%80%D0%BC%D0%B8).

- Натуралистическая индукция в сочетании с ИИ, рассматривающим гипотезу о том, что он находится в симулированной среде.

- [Логические теории принятия решений][logical_dt] и [функции полезности][utility_function], кодирующие заботу о последствиях решений ИИ через экземпляры референтного класса ИИ, которые могут быть встроены в симуляции инопланетян.

Что касается последних двух возможностей, стоит отметить, что ИИ не обязательно рассматривать возможности, в которых вся Земля, какой мы ее знаем, является симуляцией. ИИ просто нужно учитывать, что среди возможных объяснений его текущих сенсорных и внутренних данных есть сценарии, в которых он встроен в какой-то другой мир помимо самого "очевидного" из подразумеваемых сенсорными данными. См. также «‎[Отдаленные сверхинтеллекты могут навязывать вашему ИИ наиболее вероятное окружение][probable_environment_hacking]» о связанной с этим опасности, когда ИИ рассматривает возможности того, что его симулируют.

([Элиезер Юдковский][EliezerYudkowsky] выступал за то, чтобы мы в любом случае не оставляли ни один ИИ, рассматривающий отдаленные цивилизации, без экстремальных уровней безопасности и надежности, поскольку в процессе наш ИИ может встроить в себя (модель) враждебного сверхинтеллекта.)

<h2 id="Проблема_разумных_подсистем">Проблема разумных подсистем</h2>

Возможно, что наиболее эффективная система для, к примеру, распределения памяти на локальном кластере, представляет собой полноценного рефлексивного агента, имеющего модель самого себя. Или что какие-то из наиболее эффективных вариантов устройства подпроцессов в ИИ, в общем имеют те свойства, которые приводят к появлению сознания или делают личностью.

Это может представлять собой сравнительно менее серьезную моральную катастрофу в том случае, если подсистемы сознательны, но не имеют архитектуры подкрепления удовольствием/болью (неочевидно, что такая архитектура необходима для наиболее эффективных субагентов). В этом случае большое количество сознательных существ могут быть встроенными в ИИ и иногда умирать во время замены, но они не будут страдать. Тем не менее многие из нас предпочли бы избежать и такого сценария.

<h2 id="Проблема_разумных_моделей_себя">Проблема разумных моделей себя</h2>

Создаваемые ИИ модели _самого себя_ или других ИИ, которых он мог бы построить, могут оказаться сознательными или иметь другие свойства, делающие личностью. Эту возможность стоит рассматривать как отдельную от того случая, когда мы сами ненамеренно создаем сознательного или обладающего личностью ИИ, по следующим двум дополнительным причинам:

- Даже если текущее устройство ИИ не является сознательным или обладающим личностью, этот ИИ может рассматривать возможные будущие версии себя или устройства субагентов, которые сознательны, и сами его размышления о них тоже могут быть сознательными.
  - Это означает, что даже если текущая версия ИИ не кажется имеющей ключевых свойств личностности — что мы успешно создали сам ИИ как не-личность — нам все равно нужно беспокоиться о других сознательных ИИ, которые могут быть встроены в него.

- ИИ может создавать, воспроизводить и останавливать очень большое количество возможных моделей себя.
  - Даже если мы считаем допустимым возможный моральный вред создания _одного_ сознательного ИИ (например, ИИ не дотягивает до условий, которые ответственный родитель хотел бы обеспечить создаваемому им новому разумному виду, но это лишь одно разумное существо, так что ради спасения мира это будет приемлемо), возможно, мы не захотим принять на себя моральную вину за создание _триллионов_ недолговечно живущих быстро стираемых сознательных существ.

<h2 id="Сложности">Сложности</h2>

Попытки рассмотреть эти проблемы усложнены по следующим причинам:

- Философская неопределенность в отношении того, какие свойства составляют сознание и какие компьютерные программы их имеют;
- [Моральная неопределенность][moral_uncertainty] в отношении того, какая (идеализированная версия) морали (какого-либо конкретного лица) будет определять ключевые свойства личностности;
- Наша сегодняшняя неопределенность в отношении того, как будут выглядеть эффективные модели внутри продвинутых агентов.

Было бы лучше, если бы мы знали ответы на эти вопросы. Но то, что мы их не знаем, не значит, что мы можем заключить, что какая-то конкретная модель — не личность. (Это была бы какая-то смесь аргумента к незнанию и предвзятости в отношении доступности, заставляющая нас думать, что сценарий маловероятен, если его трудно представить.) В пределе при [бесконечной вычислительной мощности][unbounded_analysis] эпистемически наилучшие модели людей почти наверняка включали бы симулирование многих возможных их версий. А у сверхинтеллектуальных агентов будет очень много вычислительной мощности, и мы не знаем, в какой момент они подойдут достаточно близко к наилучшему моделированию людей, чтобы пересечь порог.

<h3 id="Масштабы_потенциальной_катастрофы">Масштабы потенциальной катастрофы</h3>

Возможность ментального преступления особенно тревожна. Поскольку достаточно продвинутые агенты, _особенно_ если они используют вычислительно эффективные модели, могут рассматривать _очень большое число_ гипотетических возможностей, которые будут личностями внутри них. Нет предела — вроде того, что если есть семь миллиардов людей, то агент будет запускать не более семи миллиардов моделей. Ведь агент может рассматривать множество вариантов для каждого отдельного человека. Это не было бы астрономической катастрофой, поскольку (гипотетически) это не уничтожило бы наших потомков и межгалактическое будущее. Но это могло бы быть бедствием на порядки ужаснее, чем Холокост, монгольские завоевания, Средние века, или все известные человеческие трагедии.

<h3 id="Проблема_порядка_разработки">Проблема порядка разработки</h3>

Если мы попросим ИИ предсказать, что бы мы сказали, если бы у нас была тысяча лет подумать о проблеме определения личности или о том, какие каузальные процессы являются "сознательными", то выглядит необычайно вероятным, что ИИ совершит ментальное преступление в процессе ответа на этот вопрос. Это выглядит таким даже в случае просьбы к ИИ поразмышлять абстрактно о проблеме сознания или предсказать с помощью абстрактных рассуждений, что люди могли бы сказать на эту тему. Так что встает проблема порядка разработки, мешающая нам попросить [Дружественный ИИ][FAI] решить для нас эту задачу, поскольку выполнить такой запрос безопасно и без совершения ментального преступления можно было бы лишь тогда, когда поставленная задача уже решена.

Перспектива крупномасштабной катастрофы препятствует идее "временно" мириться с ментальными преступлениями внутри системы, пока агенты, основанные, например, на [экстраполированном волении][cev] или одобрении, пытаются вычислить код или устройство агента, который не совершал бы ментальных преступлений. В зависимости от эффективности агента и, во вторую очередь, от его вычислительных пределов, в процессе "временного" вычисления ответа может быть уже нанесен огромный моральный ущерб.

<h3 id="Странность">Странность</h3>

Буквально никто за пределами [MIRI][MIRI] и [FHI](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D1%81%D1%82%D0%B8%D1%82%D1%83%D1%82_%D0%B1%D1%83%D0%B4%D1%83%D1%89%D0%B5%D0%B3%D0%BE_%D1%87%D0%B5%D0%BB%D0%BE%D0%B2%D0%B5%D1%87%D0%B5%D1%81%D1%82%D0%B2%D0%B0) не говорит об этой проблеме. (На самом деле это не так: эту тему уже довольно давно поднимают по меньшей мере авторы из [CLR](https://longtermrisk.org/) и [CRS](https://centerforreducingsuffering.org/) в контексте [s-рисков](https://s-risks.org/) — прим. пер.)

<h2 id="Неличностные_предикаты">Неличностные предикаты</h2>

[Неличностный предикат][nonperson_predicate] — это эффективный тест, который можем использовать мы или ИИ, чтобы определить, что какая-то компьютерная программа точно _не_ является личностью. В принципе, неличностный предикат требует наличия всего двух возможных ответов: «‎не знаю» и «точно не личность». Приемлемо, если многие программы, не являющиеся личностями, будут помечены ответом «‎не знаю», покуда ни одна личность не помечена ошибочно ответом «‎точно не личность».

Если бы приведенное выше требование было единственным, то одним простым неличностным предикатом был бы тот, который обозначает все подряд как «‎не знаю». Неявная трудность заключается в том, что неличностный предикат должен также пропускать (распознавать как не-личности) какие-нибудь программы высокой сложности, которые делают такие вещи, как «‎приемлемое моделирование людей» или «‎приемлемое моделирование будущих версий ИИ».

Помимо сценариев ментальных преступлений, [первоначальное предложение](http://lesswrong.com/lw/x4/nonperson_predicates/) Юдковского было направлено также на то, чтобы знать, когда само устройство ИИ не обладает сознанием или не является личностью.

Похоже, скорее всего будет сложно найти хороший неличностный предикат:

- Не всех философских затруднений и вычислительных трудностей можно избежать, запросив частичный список несознательных программ вместо полного списка сознательных программ. Даже если мы не знаем, какие свойства достаточны, мы должны твердо знать что-то о свойствах, которые необходимы для сознания или достаточны для неличностности.

- Мы не можем пропустить раз и навсегда ни один полный по Тьюрингу класс программ. Мы не можем сказать раз и навсегда, что безопасно моделировать гравитационные взаимодействия в солнечной системе, если огромные гравитационные системы могут кодировать компьютеры, которые кодируют людей.

- Проблема [ближайшей незаблокированной стратегии][nearest_unblocked] выглядит особенно тревожной. Если мы заблокируем какие-то варианты моделирования людей напрямую, то следующий лучший вариант с необычайно высокой вероятностью будет сознательным. Даже если мы будем полагаться на белый список, а не на черный, это может привести к белолистной "гравитационной модели", которая тайно кодирует человека, и так далее.

<h2 id="Пути_исследований">Пути исследований</h2>

- [Бихевиоризм][behaviorist]: Попробовать создать [ограниченный ИИ][limited_agi], который не строит модели других умов или, может быть, даже самого себя, за исключением некоторого узкого класса моделей агентов, которые, как мы уверены, не будут сознательными. Этот путь может быть мотивирован и другими причинами, такими как избегание [взлома вероятного окружения][probable_environment_hacking] и предотвращение манипулирования программистом.

- Попытаться определить неличностный предикат, который пропускал бы достаточно много программ для реализации с их помощью [достижений решающей значимости][pivotal].

- Попробовать создать ИИ, который мог бы загрузить наше понимание сознания и рассказать нам, что мы бы определили как личность, совершив при этом сравнительно небольшое число ментальных преступлений — так, чтобы все вычисленные возможные люди хранились, а не выбрасывались, и моделируемые агенты были полностью счастливы, по большей части счастливы, или хотя бы не страдали. Например, поместить счастливого человека в центр агента, направленного на одобрение, попробовать вести надзор над  алгоритмами ИИ, и попросить его не использовать моделирование методом Монте-Карло, если это возможно.

- Не обращать внимания на проблему на всех стадиях до освоения космоса, поскольку она все еще относительно мала по сравнению с астрономическими ставками, и поэтому не стоит значительных потерь в вероятности успеха. (Но при некоторых версиях гипотезы симуляции это может привести к обратному результату.)

- Постараться [решить][executable_philosophy] философскую задачу понимания того, какие каузальные процессы испытывают разумность (или по иным причинам являются объектами этической ценности) в ближайшие пару десятилетий — достаточно детально, чтобы это можно было четко заявить ИИ, с достаточно полным охватом, чтобы он не был подвержен проблеме [ближайшей незаблокированной стратегии][nearest_unblocked].

{% include routes.html %}

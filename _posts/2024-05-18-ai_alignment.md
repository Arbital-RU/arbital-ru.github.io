---
layout: post
title: Согласование ИИ
author: "Элиезер Юдковский"
translated_by: '<a href="https://kkirdan.github.io">К. Кирдан</a>'
excerpt: "«Проблема согласования продвинутых агентов» или «согласование ИИ» — это главная тема в исследованиях того, как разработать достаточно продвинутый машинный интеллект так, чтобы его использование приводило к хорошим последствиям в реальном мире."
---
«Проблема согласования [продвинутых агентов][advanced_agent]» или «согласование ИИ» — это главная тема в исследованиях того, как разработать [достаточно продвинутый машинный интеллект][sufficiently_advanced_ai] так, чтобы его использование приводило к [хорошим][beneficial] последствиям в реальном мире.

И «[продвинутых агентов][advanced_agent]», и «[хорошее][value_alignment_value]» стоит воспринимать как метасинтаксические заглушки на месте сложных идей, которые все еще остаются предметом дискуссий. Термин «согласование» призван передать идею направления ИИ в определенную сторону — подобно направлению в определенную сторону ракеты, которую вы построили.

По задумке, «Теория согласования ИИ» — это общий термин для всей области связанных с этой проблемой исследований, включая, например, широко обсуждаемые попытки оценить, насколько быстро ИИ сможет расширить свои возможности по мере преодоления тех или иных порогов.

Другие термины, которые использовались для описания этой исследовательской проблемы, включают «[надежный][AI_safety_mindset] и [полезный][beneficial] ИИ» и «Дружественный ИИ». Термин «[проблема согласования ценностей][value_alignment_problem]» был введен Стюартом Расселом для обозначения основной подзадачи согласования предпочтений ИИ с (потенциально [идеализированными][cev]) человеческими предпочтениями.

Некоторые альтернативные термины для этой общей области исследований, такие как «проблема контроля», могут нести слишком большой [оттенок противостояния][nonadversarial] — как будто ракета уже направлена ​​в неправильном направлении, и вам нужно с ней бороться. Другие термины, такие как «безопасность ИИ», недооценивают то, в какой степени согласованность должна быть неотъемлемой частью создания продвинутых агентов. Ведь, например, нет никакой отдельной теории «безопасности мостов» о том, как строить мосты, которые не рушатся. Вот и направление агента в нужную сторону следует рассматривать как неотъемлемую часть стандартной задачи создания продвинутого машинного агента. Проблема не делится на «создание продвинутого ИИ», и затем отдельно на «каким-то образом заставить этот ИИ давать хорошие последствия». Проблема в том, чтобы «получить хорошие последствия посредством создания когнитивного агента, который приводит к этим хорошим последствиям».

Хорошей вводной статьи или обзорной статьи по этой области в настоящее время нет. Если вы понятия не имеете, в чем заключается эта проблема, подумайте о том, чтобы почитать [популярную книгу Ника Бострома «Искусственный интеллект»][bostrom_superintelligence]<sup>*</sup>.

Вы можете изучить на Arbital данную область, перейдя по [этой ссылке][explore/ai_alignment]. См. также "[Список тем по согласованию ценностей][value_alignment_subject_list]" на Arbital, хотя он уже неактуален.

---

\*\. Книга переведена на русский язык и [издана](https://www.mann-ivanov-ferber.ru/books/iskusstvennyj-intellekt) «Манн, Иванов и Фербер» — прим. пер.

{% include routes.html %}

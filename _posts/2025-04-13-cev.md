---
layout: post
title: "Когерентное экстраполированное волеизъявление (цель согласования)"
translated_by: К. Кирдан
translation_details: "добавлены ссылки"
excerpt: "«Когерентное экстраполированное волеизъявление» (КЭВ) — это предложенная Элиезером Юдковским идея того, что делать с чрезвычайно продвинутым ОИИ, если вы абсолютно уверены в своей способности согласовать его со сложными целями."
original: https://arbital.com/p/cev
license: https://creativecommons.org/licenses/by/3.0/deed.ru
---
## Содержание
- 1\. [Введение](#1)
- 2\. [Концепция](#2)
- 3\. [Мотивация](#3)
- 4\. [Положение КЭВ в современной метаэтике](#4)
- 5\. [Пугающие проблемы проектирования](#5)
- 6\. [Что, если КЭВ не сходится?](#6)
  - 6.1\. [Помощь людям с некогерентными предпочтениями](#6.1)
- 7\. [Роль метаидеалов в продвижении раннего соглашения](#7)
  - 7.1\. [Роль «когерентности» в уменьшении ожидаемых неразрешимых разногласий](#7.1)
- 8\. [Либо моральная опасность, либо исправление ошибок](#8)
- 9\. [Проблема «эгоистичных мерзавцев»](#9)
- 10\. [Почему основу КЭВ составляют «ныне живущие люди», а не какой-нибудь другой класс экстраполируемых?](#10)
  - 10.1\. [Почему бы не включить млекопитающих?](#10.1)
  - 10.2\. [Почему бы не включить всех разумных существ?](#10.2)
  - 10.3\. [Почему бы не включить умерших людей?](#10.3)
  - 10.4\. [Зачем включать бессильных людей?](#10.4)

---

<h2 id="1">Введение</h2>

«Когерентное экстраполированное волеизъявление» (КЭВ) — это предложенная [Элиезером Юдковским][EliezerYudkowsky] идея того, что делать с чрезвычайно [продвинутым ОИИ][advanced_agent], если вы абсолютно уверены в своей способности [согласовать][value_alignment_problem] его со сложными целями.

Грубо говоря, сверхинтеллект на основе КЭВ делал бы то, чего хотели бы\* существующие в данный момент люди, _если бы, гипотетически_:

- Мы знали бы всё, что знает ИИ;
- Мы могли бы думать так же быстро, как ИИ, и рассматривать все аргументы;
- Мы идеально знали бы себя и обладали бы лучшим самоконтролем или способностью к самомодификации;

..._до такой степени_, в какой большинство экстраполированных вот так существующих людей предсказуемо хотели\* бы одного и того же. (Например, в пределе экстраполяции почти все люди, вероятно, не хотели\* бы быть [превращёнными в скрепки][paperclip_maximizer], но могли бы иметь разногласия\* насчет того, какова лучшая начинка для пиццы. См. ниже.)

КЭВ задумано как нечто _буквально оптимальное_, _идеальное_ или _нормативное_, что стоило бы предпринять с [автономным сверхинтеллектом][Sovereign] в том случае, если вы доверяете своей способности [идеально согласовать][total_alignment] сверхинтеллект с очень сложной целью. (См. ниже.)

КЭВ довольно сложно и ориентировано на метауровень, поэтому _не_ предназначено для использования с самым первым ИИ, который вы попытаетесь создать. КЭВ может стать тем, что все участники проекта согласились бы принять как приемлемую общую цель для их _второго_ ИИ. (Первым ИИ, вероятно, должен был бы быть [ИИ-для-поручений][task_agi].)

Для соответствующей метаэтической теории см. «[Экстраполированная воля (нормативная моральная теория)][normative_extrapolated_volition]».

<h2 id="2">Концепция</h2>

[Экстраполированное волезъявление][normative_extrapolated_volition] — это метаэтическая теория, согласно которой, когда мы спрашиваем «что правильно?», то, в той мере, в какой этот вопрос имеет какой-либо смысл, мы на самом деле спрашиваем: «чего бы хотела\* гипотетическая идеализированная версия меня, если бы знала все факты, рассмотрела все аргументы и обладала совершенным самопознанием и самоконтролем?»[^1] (Как [метаэтическая теория][normative_extrapolated_volition], этот подход делает вопрос «что правильно?» смесью логического и эмпирического вопросов, функцией от возможных состояний мира.)

Приведем простой пример экстраполированного волеизъявления. Пусть кто-то просит вас принести апельсиновый сок из холодильника. Вы открываете холодильник и видите, что апельсинового сока там нет, но есть лимонад. Вы представляете, что ваш друг захотел бы лимонада, если бы знал все, что вы знаете о содержимом холодильника, так что вы приносите ему лимонад. На абстрактном уровне можно сказать, что вы «экстраполировали» «волеизъявление» своего друга: вы взяли свою модель его разума и процесса принятия решений, или свою модель его «волеизъявления», и вообразили гипотетическую версию его разума с лучшими знаниями о содержимом холодильника, таким образом «экстраполировав» это волеизъявление.

Обладание лучшей информацией — это не единственный способ экстраполировать процесс принятия решений; можно также, например, представить, что разум имеет больше времени для обдумывания моральных аргументов или лучшее знание о самом себе. Возможно, сейчас вы хотите отомстить семье Капулетти, но если бы у кого-то была возможность поговорить с вами о том, как месть влияет на цивилизации в долгосрочной перспективе, вас можно было бы убедить отказаться от этой идеи. Или, может быть, вы убеждены, что выступаете за запрет зеленых туфель из благих побуждений, но если бы вы могли увидеть распечатку со всеми своими эмоциями, задействованными в этом вопросе, вы бы поняли, что в вас много негатива по отношению к людям, которые носят зеленые туфли, и это изменило бы ваше мнение об этом решении.

В версии Юдковского выделяются три основных направления экстраполяции, рассматриваемой на индивидуальном уровне:

- Увеличение знания — получение большего количества достоверных знаний об описательных фактах и ожидаемых исходах.
- Увеличение рассмотрения аргументов — способность рассматривать больше возможных аргументов и оценивать их обоснованность.
- Увеличение рефлексивности — лучшее знание о себе и в некоторой степени улучшенный самоконтроль (хотя это поднимает дополнительные вопросы о том, какие части «я» какие другие части нормативно должны контролировать).

<h2 id="3">Мотивация</h2>

Разные люди по-разному реагируют на вопрос «на что мы должны направить сверхинтеллект?» или «что должен делать согласованный сверхинтеллект?», когда впервые на него отвечают — не только из-за разных представлений о том, что есть благо, но и из-за разных подходов к тому, как вообще задавать вопрос.

Вот несколько распространенных реакций:

- «Разные люди хотят разных вещей! Невозможно угодить всем. Даже если ты выберешь какой-то способ комбинировать желания людей, _именно ты_ будешь решать, как это сделать. Кто-то другой может просто решить, что заслуживает править всем миром. В итоге, это _ты_ решаешь, что будет делать ИИ, и любые утверждения о какой-то высшей справедливости или нормативности — это просто софистика.»
- «То, что нам нужно от ИИ, очевидно: он должен оптимизировать либерально-демократические ценности. Это уже учитывает интересы всех лиц справедливым образом. Реальная угроза — если плохие люди получат ИИ в свои руки или создадут ИИ, который не будет оптимизировать либерально-демократические ценности.»
- «Представьте себе, что древние греки говорят сверхинтеллекту, что делать: они велели бы ему оптимизировать славные смерти в бою. Задавать сверхинтеллекту любые другие наборы жестко зафиксированных целей не менее глупо. Ему нужно уметь меняться и расти.»
- «Что, если мы скажем сверхинтеллекту, что делать, и это окажется неправильным? Что, если мы вообще заблуждаемся насчет того, что правильно? Разве не лучше будет позволить сверхинтеллекту самому разобраться в этом, учитывая его предполагаемое интеллектуальное превосходство?»

Первоначальный ответ на каждый из этих подходов может быть таким:

- «Хорошо, но представьте, что вы создаете сверхинтеллект и стараетесь _не быть негодяем в этом деле_. Если вы решаете "все, что я делаю, исходит из меня самого, и поэтому одинаково эгоистично, так что я могу просто провозгласить себя Богом-Императором Вселенной", то вы ведете себя как негодяй. Можно ли сделать что-то другое вместо этого, чтобы не быть негодяем? Что было бы _наименее_ негодяйским?»
- «А что, если после дальнейшего обсуждения вы захотели бы немного изменить свое определение "либерально-демократических ценностей"? Что, если можно _предсказать_, что вы это сделаете? Вы действительно хотели бы застрять с вашей поспешной версией определения этих ценностей на следующий миллион лет?»
- «Хорошо, но что вместо этого древние греки должны были бы сделать, если бы им пришлось программировать ИИ? Как они могли бы не обречь будущие поколения? Допустим, греки достаточно умны, чтобы заметить, что люди иногда меняют свое мнение, и чтобы понять, что они могут быть не во всем правы. Как они могли бы использовать ум ОИИ конструктивно описанным вычислимым путем, чтобы выбраться из этой ситуации? Нельзя просто сказать ИИ "вычисли, что правильно", нужно задать конкретный вычислимый вопрос, а не просто набор слов.»
- «Вы спросили: что, если мы вообще заблуждаемся насчет того, что правильно? Но тогда что вообще значит слово "правильно"? Если вы не знаете, что правильно, и не знаете, как это вычислить, то о чем мы вообще говорим? Есть ли у вас хоть одно основание сказать, что ИИ, который стремится просто создать как можно больше скрепок, не вычисляет правильность? Если вы считаете, что максимизация скрепок не вычисляет правильность, то вы, должно быть, уже что-то знаете о том, как вычислять правильность, что позволило вам отбросить неверный путь. Так давайте обсудим, как запрограммировать вопрос о вычислении правильности в ИИ.»

Сторонники КЭВ утверждают, что все эти рассуждения в конечном итоге приводят к идее когерентного экстраполированного волеизъявления. Например, так:

- Спросить, чего хотел\* бы каждый, если бы знал всё, что знает ИИ, и сделать то, на что они все предсказуемо согласились бы — это примерно наименее негодяйское, что вы могли бы сделать. Если вы попросите ИИ дать каждому по вулканической пещере, потому что лично вам нравятся вулканические пещеры, это не будет эгоистичным, но вы все еще будете негодяем по отношению к тем, кто не хотел бы жить в такой пещере. Если вы будете просто выполнять то, что люди говорят, то они причинят себе вред неразумными желаниями, и вы снова будете негодяем. А если вы экстраполируете желания только ваших друзей и ИИ будет выполнять только то, что вы хотите, вы будете негодяем по отношению ко всему остальному миру.
- Да, либерально-демократические ценности — это хорошо. Как и яблочный пирог. Яблочный пирог — это хорошо, но это _не единственная_ хорошая вещь. Список конечных целей Уильяма Франкены включал «жизнь, сознание и деятельность; здоровье и сила; удовольствие и удовлетворенность всех или определенных видов; счастье, блаженство, довольство» и еще 25 пунктов. И этот список, разумеется, неполон. Единственный способ получить полный список — анализировать человеческие умы. И даже с таким списком, если наши потомки через миллион лет предсказуемо захотят чего-то другого, мы тоже должны будем это учесть.
- Каждое улучшение — это изменение, но не каждое изменение — улучшение. Позволить сверхинтеллекту просто меняться случайным образом — не значит добиться морального прогресса. А говорить, что движение к либерально-демократическим ценностям — это прогресс, значит предполагать, что мы уже знаем конечный пункт назначения. Мы не можем даже просто попросить ИИ _предсказать_, что цивилизация будет думать через тысячу лет, потому что (а) сам ИИ влияет на этот итог, а (б) если ИИ ничего не сделает, возможно, через тысячу лет все случайно превратятся в торчков, пытаясь модифицировать свои собственные мозги. Если мы хотим поступить лучше, чем гипотетические древние греки, нам нужно определить _достаточно абстрактный и метауровневый критерий_, который описывал бы _валидные_ направления прогресса — такие, как изменения моральных убеждений в результате познания новых фактов о мире, предсказуемые изменения в результате рассмотрения новых аргументов, а также предсказуемые изменения в результате нашего лучшего понимания самих себя.
- Это долгая история. Метаэтика занимается вопросом о том, что именно представляет собой «правильность» — пытается примирить это странное, неуловимое понятие «правильности» с физическим миром, состоящим из полей элементарных частиц. Хотя кажется, что если люди хотят убивать других людей, это ещё _не делает_ убийство правильным, нигде во вселенной не прописано, что убийство — это неправильно. После довольно длительных обсуждений мы пришли к выводу, что для любого конкретного человека в любой конкретный момент времени, «правильность» — это логическая константа, которая, хотя и не зависит контрфактически от состояния мозга этого человека, должна аналитически соотноситься с экстраполированной волей этого мозга. Мы показываем, что (лишь) такая позиция дает согласованные ответы на все стандартные вопросы метаэтики. (Обсуждение этого вопроса заняло бы много времени — сравнимо с объяснением того, как из детерминированности законов физики не следует, что у вас нет свободной воли.)

<h2 id="4">Положение КЭВ в современной метаэтике</h2>

См. соответствующий раздел в статье «[Экстраполированное волеизъявление (нормативная моральная теория)][normative_extrapolated_volition]».

<h2 id="#5">Пугающие_проблемы_проектирования</h2>

Есть ряд причин, по которым КЭВ _слишком_ сложно, чтобы быть хорошей целью для первой попытки в любом проекте построения машинного интеллекта:

- КЭВ-агент должен будет [автономно][Sovereign] выполнять бессрочную миссию. Это влечет все те обычные проблемы, из-за которых, как мы ожидаем, автономный ИИ сложнее сделать безопасным, чем [ОИИ-для-поручений][task_agi].
- КЭВ — странная цель, вовлекающая рекурсию.
- Даже такие понятия в КЭВ, как «знать больше» или «экстраполировать человека», выглядят сложными и [ценностно-нагруженными][value_laden]. Возможно, потребуется сначала создать высокоуровневого агента типа «[Do What I Know I Mean][dwim]» (делай то, что я знаю, что имею в виду), а затем поручить ему выполнять КЭВ. А этот агент уже сам по себе столь сложен, что вам придется создать ИИ, который мог бы обучаться DWIKIM, так как это не получится задать формально. Таким образом, мы имеем что-то вроде следующего: КЭВ работает поверх DWIKIM, а тот работает поверх системы обучения целям — по крайней мере до тех пор, пока КЭВ-агент не перепишет сам себя в первый раз.

Кажется _маловероятным_, что _при первой же попытке_ создания интеллекта, превосходящего человеческий, можно правильно реализовать КЭВ. Единственный случай, в котором это могло бы стать хорошей первоочердной целью, — это если бы концепция КЭВ оказалась формально проще, чем она сейчас выглядит, а времени до ОИИ было бы необычно много, что позволило бы провести множество предварительных исследований по безопасности.

Если ОИИ появится через 20 лет (или раньше), то, кажется, мудрее думать об [ОИИ-для-поручений][task_agi], который выполнял бы какую-то сравнительно простую задачу [решающей значимости][pivotal]. Роль же КЭВ — ответить на вопрос: «Если вы все сможете договориться об этом заранее, то что именно вы попытаетесь сделать после того, как завершите дела с ОИИ-для-поручений и избавитесь от угрозы скорой гибели?»

<h2 id="#6">Что, если КЭВ не сходится?</h2>

Часто задаваемый вопрос: «что, если экстраполяция человеческих воль приводит к некогерентным результатам?»

Согласно исходной мотивации КЭВ, если _в каких-то местах_ нет сходимости, дружественный ИИ должен их игнорировать. Если же это происходит _повсеместно_, то вы, наверное, выбрали неверный способ построения экстраполированного волеизъявления, и вам нужно его пересмотреть[^2].

Иными словами:

- Если ваш алгоритм КЭВ обнаруживает, что «люди когерентно не хотят быть съеденными [максимизаторами скрепок][paperclip_maximizer], но при этом имеют широкий спектр разных предпочтений о начинках для пиццы», то мы нормативно хотели бы, чтобы дружественный ИИ предотвратил поедание людей максимизаторами скрепок, но не вмешивался в то, какие начинки для пиццы люди будут есть в будущем.
- Если ваш алгоритм КЭВ утверждает, что нет никакого когерентного смысла, в котором «многие люди не хотели бы быть съеденными Скрепочником, и их желание\* не поменялось бы, даже если бы они знали больше», то это подозрительный и неожиданный результат. Возможно, вы выбрали неправильный способ интерпретации чьей-либо воли.

Изначальную мотивацию КЭВ можно еще рассмотреть с точки зрения вопросов «что значит помочь человеку?» и «как можно помочь большой группе людей?». Цель — создать ИИ, который оказывал бы «помощь» так, как мы её [действительно понимаем][intended_goal]. Элементы КЭВ могут рассматриваться как оговорки к наивному представлению о том, что «помощь — это дать людям всё, что они попросят!», которое допускает ситуацию, когда кто-то просит вас принести апельсиновый сок, и вы приносите отравленный из холодильника (в то время как человек _не пытался_ себя отравить).

А что насчёт помощи группе людей? Если двое просят сок, а вы можете принести только один вид сока, то нужно принести неотравленный сок, который понравился бы обоим, насколько это возможно. Если такого нет, найдите сок, который понравился бы одному, и к которому был бы равнодушен другой, — бросив монетку или вроде того, чтобы определить, на чей вкус ориентироваться. В данных обстоятельствах это будет максимальной помощью.

Возможно ли, что нет вообще никакого способа помочь большой группе людей? Это кажется маловероятным. Вы могли бы хотя бы накормить голодных пиццей с той начинкой, которая им сейчас нравится. Если ваша философия утверждает: «О нет, даже _это_ — не помощь, потому что она не идеально когерентна», значит, вы выбрали неверную интерпретацию понятия «помощи».

Возможно, если мы обнаружим, что все разумно звучащие интерпретации экстраполированной воли некогерентны, нам придётся перейти к какой-то совсем другой концепции «помощи». Но и тогда новая форма помощи не должна включать в себя раздачу отравленного апельсинового сока людям, которые не знают, что он отравлен, — потому что это в любом случае интуитивно не кажется помощью.

<h3 id="#6.1">Помощь людям с некогерентными предпочтениями</h3>

Что, если кто-то считает, что [предпочитает лук ананасам в качестве начинки для пиццы, ананасы грибам, а грибы луку][intro_utility_coherence]? В том смысле, что если предложить ему выбрать любые два кусочка из этого набора, он будет выбирать в соответствии с данным упорядочиванием?

(Этот пример не является нереалистичным. Многочисленные эксперименты по поведенческой экономике демонстрируют как раз такие круговые предпочтения. Например, можно разложить 3 предмета так, чтобы при сравнении каждой отдельной пары фокус внимания падал на разные их качества.)

Можно беспокоиться, что мы не сможем «когерентно экстраполировать волю» человека с такими предпочтениями насчет начинок для пиццы, поскольку эти отдельные предпочтения явно не согласуются ни с какой когерентной функцией полезности. Но как же можно _помочь_ человеку, если у него такие предпочтения?

Ну, апеллируя к интуитивному понятию _помощи_:

- Мы могли бы дать ему тот вид пиццы, который он выбрал бы, если бы ему пришлось выбирать из всех трёх одновременно.
- Мы могли бы выяснить, насколько человек был бы счастлив от съедения каждого вида пиццы в плане эмоциональной интенсивности (как если бы ее измеряли с помощью нейротрансмиттеров), и предложить ему тот кусок, который принесёт ему наибольшее удовольствие.
- Мы могли бы позволить человеку самому выбирать его чертову начинку для пиццы, а сами позаботились бы лишь о том, чтобы пицца не была ядовитой, ведь человек явно предпочитает неядовитую пиццу.
- Мы могли бы, имея достаточные умственные ресурсы, выяснить, что этот человек попросил бы нас сделать для него в данной ситуации после того, как он узнал бы о концепции изменения предпочтений и был бы проинформирован о своих круговых предпочтениях. А если результат сильно варьируется в зависимости от того, как именно мы объясняем концепцию изменения предпочтений, мы могли бы вернуться к одному из предыдущих трёх вариантов.

С другой стороны, следующие варианты кажутся _менее соответствующими_ понятию помощи:

- Отказаться иметь дело с этим человеком, поскольку его текущие предпочтения не формируют когерентную функцию полезности.
- Издавать звуки «ОШИБКА, ОШИБКА», как ИИ из голливудских фильмов, который только что узнал о парадоксе Эпименида.
- Дать ему пиццу с вашей любимой начинкой, зелёным перцем, даже если ему больше понравилась бы любая из трёх других начинок.
- Дать ему пиццу с начинкой, которая ему больше всего понравится по вкусу, пепперони, несмотря на то, что он вегетарианец.

Сторонники КЭВ утверждают, что если вы отбросите сложности экстраполированного волеизъявления и зададитесь вопросом, как можно было бы разумным путем как можно сильнее помочь людям, стараясь при этом не быть негодяем, а затем попробуете примерно формально описать тот мысленный процесс, который вы использовали для ответа на этот вопрос, то в конечном итоге вы снова вернетесь к КЭВ.

<h2 id="#7">Роль метаидеалов в продвижении раннего соглашения</h2>

Основная цель КЭВ — представить сравнительно простой метауровневый идеал, насчет которого люди могли бы согласиться, даже если они расходятся во мнениях на объектном уровне. Возможно, аналогичным примером может служить ситуация, когда двое честных учёных могут иметь разногласие насчет точной массы электрона, но соглашаться в том, что экспериментальный метод — хороший способ решить этот вопрос.

Представьте себе, что Милликен считает, что масса электрона составляет 9,1e-28 граммов, а Нанникен считает, что верная масса электрона составляет 9,1e-34 граммов. Милликена может сильно беспокоить предложение Нанникена запрограммировать ИИ, чтобы он считал, что масса электрона составляет 9,1e-34 граммов, а Нанникену не нравится предложение Милликена запрограммировать убеждение ИИ на 9,1e-28 граммов, и они оба будут недовольны компромиссной массой в 9,1e-31 граммов. Тем не менее, они могут согласиться запрограммировать в ИИ аналоги теории вероятностей и принципа простоты, и позволить сверхразуму прийти к выводам, вытекающим из Байеса и Оккама. Потому что они оба могут согласиться с тем, каким должен быть вычислимый вопрос, даже если их предполагаемые ответы на этот вопрос различаются. Конечно, легче прийти к такому соглашению, пока ИИ ещё не выдал ответ, или если ИИ не сообщает его вам.

Нет гарантии, что каждый человек воплощает одни и те же имплицитные моральные вопросы. В самом деле, это кажется маловероятным, поэтому Алиса и Боб могут ожидать, что даже после экстраполяции их воли будут расходиться по некоторым вопросам. Однако, покуда результаты остаются абстрактными и ещё не вычислены, Алисе трудно убедить Кэрол, Денниса и Эвелин, что, с точки зрения морали и справедливости, ИИ должен реализовать именно её экстраполированную волю, а не волю Боба. Чтобы убедить Кэрол, Денниса и Эвелин, что это так, Алисе нужно было бы, чтобы они верили, что её ЭВ с большей вероятностью будет в согласии с их ЭВ, чем воля Боба. Учитывая это, почему бы всем вместе не остановиться на очевидной точке Шеллинга, заключающейся в экстраполяции воли _каждого_ человека?

Таким образом, одно из главных назначений КЭВ (его привлекательных сторон и целей проекта) состоит в том, что Алиса, Боб и Кэрол могут уже сейчас договориться о том, что Деннис и Эвелин должны будут делать с ИИ, который будет разработан позднее. Мы можем уже сейчас попытаться установить механизмы обязательств или сдержек и противовесов, чтобы гарантировать, что Деннис и Эвелин в будущем всё ещё будут работать над КЭВ.

<h3 id="#7.1">Роль «когерентности» в уменьшении ожидаемых неразрешимых разногласий</h3>

КЭВ не обязательно представляет собой выбор решений, за которые голосует большинство. Большое количество людей со слабой экстраполированным предпочтением\* может быть уравновешено небольшим количеством людей с сильным экстраполированным предпочтением\*, направленным в противоположную сторону. Модель «[парламента](http://www.overcomingbias.com/2009/01/moral-uncertainty-towards-a-solution.html)», предложенная Ником Бостромом для разрешения неопределённости между несоизмеримыми этическими теориями, позволяет подтеории, особенно обеспокоенной каким-либо решением, потратить значительную часть своего ограниченного влияния на то, чтобы повлиять именно на это решение.

Это значит, что, например, вегану или зоозащитнику не следует думать, что для того, чтобы результат КЭВ защищал животных, им нужно захватить контроль над алгоритмом. Кажется маловероятным сценарий, в котором после создания сверхинтеллекта нанесение вреда животным приносило бы большей части человечества огромное количество полезности. Так что для предотвращения такого сценария достаточно даже того, чтобы небольшая часть популяции была сильно против\* него.

(ДОБАВЛЕНО В 2023: Томас Седерборг справедливо отмечает, что оригинальное парламентское предложение Ника Бострома включает в себя в качестве отправной точки для переговоров случайный шанс каждого участника стать диктатором, что даёт чрезмерное и потенциально фатальное количество власти «спойлерам» — агентам, которые искренне, а не в качестве тактического хода, предпочитают инвертировать функции полезности других агентов или делать что-то, что минимизирует эти функции полезности — если большинство участников имеет функции полезности с тем, что я называю «отрицательным перекосом». Т. е. противостоящий вам агент может использовать то же количество ресурсов, чтобы создать -100 утилонов, в то время как согласованный с вами агент может использовать его максимум для создания +1 утилона. При таких условиях, если тролли составляют 1% населения, они могут потребовать, чтобы все ресурсы были использованы так, как они хотят — в обмен на то, что они не причинят вреда. Или, проще говоря, если 1% населения предпочли бы создать ад для всех, кроме себя (и это их искреннее предпочтение, а не стратегия переговоров), и этот ад в 100 раз хуже, чем была бы инверсия рая, то если вы используете парламентскую процедуру со случайной диктатурой в качестве отправной точки, они могут захватить всё будущее. Я согласен с Седерборгом, что это более чем достаточная причина не начинать со случайной диктатуры в качестве отправной точки для переговоров. Любой, кто недостаточно умен, чтобы понять этот момент и возмутиться по этому поводу (возможно, включая Элиезера), недостаточно умен и благонадежен и для реализации КЭВ. Но попытка реализации КЭВ, вероятно, в любом случае была бы плохой идеей для нынешних людей, даже если бы у них было много времени для размышлений на их уровне интеллекта. — [Элиезер Юдковский][EliezerYudkowsky])

<h2 id="#8">Либо моральная опасность, либо исправление ошибок</h2>

Один из мотивов предложения КЭВ — минимизация [моральной опасности][moral_hazard] (т. е. чтобы у программистов не было искушения захватить власть над миром или будущим). Однако эта цель может быть подорвана, если результаты КЭВ не будут оставаться полностью непроверенными.

Часть смысла КЭВ состоит в ответе на вопрос: «Если бы древние греки первыми изобрели сверхинтеллект, что они могли бы сделать, чтобы это не привело к необратимому искажению будущего с нашей точки зрения? Если бы древние греки запрограммировали свои ценности напрямую, они запрограммировали бы славную смерть в бою. Теперь давайте предположим, что мы тоже не так уж мудры». Мы можем представить, что древние греки записали бы механизм КЭВ, заглянули в результаты его работы до того, как их реализовать, и пришли бы в ужас от отсутствия славных смертей в бою в будущем и будущей системе ценностей, которые выдал этот механизм.

Также мы можем представить, что греки, пытаясь снизить [моральную опасность][moral_hazard], добродетельно отказываются подглядывать за результатами. Но оказывается, что их попытка реализовать КЭВ привела к непредвиденным последствиям при фактическом исполнении сверхинтеллектом, и в итоге их мир превратился в скрепки.

Это вопрос баланса между безопасностью ИИ и моральной опасностью: (а) польза от возможности взглянуть на результаты КЭВ, чтобы лучше обучить систему или просто убедиться, что нет жутких неполадок, и (б) моральная опасность, вызванная искушением изменить результат, что свело бы на нет сам смысл механизма КЭВ.

Есть также потенциальный риск опасности даже от взгляда на внутреннюю работу алгоритма КЭВ. Смоделированное будущее может содержать всевозможные когнитивные угрозы, способные напрямую манипулировать разумом.

Вместо того, чтобы совсем сдаться и принять максимальную моральную опасность, одним из возможных подходов к этой проблеме может быть назначение одного человека, который должен заглянуть в результат и выдать суждение в виде 1 или 0 (продолжение либо остановка) через механизм, не передающий программистам никакой другой информации в случае выдачи 0. (Например, доброволец может находиться в комнате со взрывчаткой, которая сдетонирует в случае выдачи 0.)

<h2 id="#9">Проблема «эгоистичных мерзавцев»</h2>

Предположим, что Фред финансирует Грейс для работы над сверхинтеллектом, основанным на КЭВ, а Эвелин решила не противодействовать этому проекту. Предполагается, что результирующее КЭВ будет экстраполировать волю Алисы, Боба, Кэрол, Денниса, Эвелин, Фреда и Грейс с равным весом. (Если вы это читаете, то, скорее всего, вы Эвелин, Фред или Грейс.)

Эвелин, Фред и Грейс могут беспокоиться: «Что, если подавляющее большинство людей состоит из "эгоистичных\* мерзавцев", чья экстраполированная воля с радостью проголосует\* за мир, в котором разрешено владеть искусственными разумными существами как рабами, при условии, что сами они окажутся в классе рабовладельцев; а мы, Эвелин, Фред и Грейс, просто окажемся в меньшинстве, которое категорически не хочет и не будет хотеть\* такого будущего?»

То есть: что, если экстраполированные воли человечества расходятся настолько, что с точки зрения _нашей_ воли (поскольку, если вы это читаете, вы, скорее всего, Эвелин, Фред или Грейс) 90% экстраполированного человечества выбрали\* бы что-то такое, чего _мы_ не одобрили бы, и наши воли не одобрили\* бы, _даже_ с учётом того, что мы не хотим быть козлами и не считаем, что родились с каким-то необычным или исключительным правом определять судьбу человечества.

Т. е. пусть сценарий будет таким:

> 90% людей (но не мы, кто коллективно спонсирует этот ИИ) по своей сути — эгоистичные мерзавцы, такие, что _любой разумный_ процесс экстраполяции (дело не в том, что мы выбрали какой-то неправильный) приведёт к поддержке ими такого мира, в котором у них самих есть права, но при этом допустимо создавать искусственных людей и причинять им страдания. Более того, они получили бы столько полезности от становления персональными Богами-Императорами, что это перевесило бы наше возражение от меньшинства даже в рамках парламентской модели.

Мы можем рассматривать этот гипотетический исход как подрывающий любые причины, по которым мы, имея власть предотвратить его, должны добровольно передавать власть остальным 90% человечества:

- Мы не можем придавать приоритет справедливому отношению к каждому, включая оставшиеся 90% человечества. Потому что как насчёт справедливости к искусственным существам, которым причиняют вред?
- Мы не можем беспокоиться о том, что оставшиеся 90% человечества откажут проекту в поддержке или о том, что мы предадим его сторонников. Потому что по условиям они и так не поддерживали его и даже не давали разрешения на его существование.
- Мы не можем согласиться отложить спор до применения более правильного и разумного процесса разрешения разногласий. Потому что по условиям КЭВ, состоящее из 90% эгоистичных\* мерзавцев, не является, с нашей точки зрения, действительно более правильным.
- Мы не можем полагаться на парламентскую модель когерентности, чтобы предотвратить то, что меньшинство видит как катастрофу. Потому что по условиям остальные 90% получат достаточно много полезности от коллективного провозглашения себя Богами-Императорами, чтобы перевесить даже сильные голоса меньшинства против этого.

Вместо того, чтобы совсем сдаться и сосредоточиться на захвате мира, _или_ подвергать себя моральной опасности, подглядывая в результаты, можно подойти к этой проблеме с помощью следующего трёхэтапного процесса.

В этом процессе используются внутренние отсылки, так что сначала мы изложим его кратко, а затем более подробно.

Вкратце:

- Постройте КЭВ для всех.
- Постройте КЭВ только для участников, и пусть оно даст (только) голос «за/против» общего КЭВ.
- Если результат положительный, запускаем КЭВ для всех.
- В противном случае постройте КЭВ всех за исключением тех частей, которые, если бы они оказались в позиции неограниченной власти, действовали бы односторонне и без какой-либо заботы о предпочтениях других людей.
- Пусть КЭВ участников даст ответ «за/против» резервного КЭВ.
- Если результат положительный, запускаем резервное КЭВ.
- В противном случае Неудача.

Теперь подробно:

- Сначала строим КЭВ всех людей на Земле, как если бы его не проверяли.
  - Если какой-то гипотетический экстраполированный человек беспокоится о том, что его проверяют, удалите это беспокойство и экстраполируйте его так, как если бы у него его не было. Это нужно для того, чтобы сама проверка не оказала влияния на экстраполяцию и фактическое будущее в соответствии с UDT.
- Затем экстраполируйте КЭВ всех, кто внес вклад в проект, взвешивая по величине их вклада (возможно, на основе комбинации факторов: «сколько реально было сделано», «сколько рационально ожидаемо было сделать» и «доля того, что могло быть сделано, по сравнению с тем, что было реально сделано»). Позвольте этому второму экстраполяционному процессу проголосовать (а не внести какие-либо детализированные предложения) «за» или «против» того, следует ли запускать КЭВ всех людей на Земле без изменений.
  - Уберите из КЭВ вкладчиков все стратегические соображения, касающиеся вопроса о том, что резервное КЭВ или перестройка после Неудачи были бы _лучшей_ альтернативой. Нам нужно извлечь в каком-либо смысле «сатисфаизирующее» суждение о том, не является ли общее КЭВ недопустимо ужасным.
- Если общее КЭВ прошло проверку от КЭВ вкладчиков, запускаем его.
- В противном случае, экстраполируйте резервное КЭВ. За его основу берутся все существующие люди, но из экстраполяции _исключаюся_ все те экстраполированные процессы принятия решений, которые, если бы они находились в превосходящей стратегической позиции или имели одностороннюю власть, _отказались бы_ экстраполировать воли других людей или заботиться об их благополучии.
  - Снова уберите все экстраполированные _стратегические_ соображения о предстоящей проверке.
- Проверьте резервное КЭВ через голосование «за» или «против» от КЭВ вкладчиков. Если оно прошло проверку, запускаем его.
- В противном случае — Неудача (ИИ безопасно отключается, после чего мы переосмысляем, что делать дальше, или реализуем какой-то резервный план, на который ранее договорились).

Особенность резервного варианта, в котором мы решили «исключить из экстраполяции любые взвешенные части экстраполированных процессов принятия решений, которые действовали бы односторонне и без заботы о других, если бы получили неограниченную власть» состоит в том, что это своего рода поэтическая справедливость, отражающая возражения к этому варианту как подрывающие сами себя: если действовать односторонне допустимо, то почему мы не можем в одностороннем порядке исключить односторонне действующие элементы? Это представляется «самым простым» или «наиболее элегантным» способом исключить те части КЭВ, чьи внутренние рассуждения прямо противоречат самой изначальной причине, по которой мы запустили КЭВ. Для остальных частей такой подход влечет минимально возможные ограничения.

Таким образом, если Алиса (которая, по условиям, не относится к вкладчикам) скажет: «Но я требую, чтобы вы альтруистически включили мою экстраполяцию, которая односторонне действовала бы против вас, если бы у неё была власть!», то мы ответим: «Попробуем это сделать, но если выяснится, что это достаточно плохая идея, то у вас не будет никаких когерентных межличностных аргументов, чтобы упрекнуть нас за выбор резервного варианта вместо этого».

Также и насчет варианта Неудачи в конце, если кто-то скажет: «Справедливость требует, чтобы вы запустили резервное КЭВ, даже если вам это не понравится\*!», мы можем ответить: «Наша собственная сила не может быть использована против нас; если мы будем сожалеть о том, что вообще построили эту систему, справедливость не обязывает нас её запускать.»

<h2 id="#10">Почему основу КЭВ составляют «ныне живущие люди», а не какой-нибудь другой класс экстраполируемых?</h2>

Часто задаются такие вопросы о деталях реализации КЭВ:

- Почему КЭВ формулируется так, что экстраполируются «все ныне живущие люди», а не «все люди, живущие ныне и жившие в прошлом» или «все млекопитающие» или «все разумные существа, которые, вероятно, существуют в бесконечной мультивселенной, взвешенные по их распространенности»?
- Почему бы не ограничить основу для экстраполяции только «людьми, которые внесли вклад в проект ИИ»?

В частности, спрашивалось, почему ограничительные ответы на первый вопрос [не подразумевают][prove_too_much] более ограничительных ответов на второй.

<h3 id="#10.1">Почему бы не включить млекопитающих?</h3>

Начнем с рассмотрения некоторых ответов на вопрос «Почему бы не включить всех млекопитающих в основу для КЭВ?»

- Потому что вы можете ошибаться насчет того, что млекопитающие являются такими объектами значимой этической ценности, чтобы на объектном уровне мы _должны_ были бы уважать их благополучие. И процесс экстраполяции уловит ошибку, если вы предсказуемо измените своё мнение по этому вопросу. Включение млекопитающих в основу для КЭВ может закрепить то, что может быть ошибкой, которую мы предсказуемо обнаружили бы в будущем. Если вы нормативно _правы_ насчет того, что мы все должны заботиться о млекопитающих и даже пытаться экстраполировать их воли, чтобы определить судьбу Земли, и если почти все из нас предсказуемо приняли бы это решение после раздумий, то именно это и решат\* сделать наши ЭВ от нашего имени; а если они не примут\* такого решения, значит, оно и не было правильным, что подрывает ваш аргумент о том, что это нужно было сделать безо всяких условий.
- Потому что даже если мы должны заботиться о благополучии млекопитающих, их экстраполированные версии могут оказаться такими чертовски странными, что вы пожалеете, что включили их в КЭВ. (Например, после того как человеческие воли будут перебиты экстраполированными волями других животных, текущая база экстраполированных воль животных решит\* создать мир, где они будут возвышены до уровня Богов-Императоров и править другими страдающими животными.)
- Потому что, возможно, не все на Земле заботятся\* о животных, даже если ваша ЭВ в самом деле заботилась\* бы о них. И чтобы избежать схваток за мировое господство, мы разрешим этот вопрос, например, по модели типа парламента, в котором вы сможете использовать свою долю влияния в решении дальнейшей судьбы Земли для защиты животных.

Последнее соображение можно расширить, ответив: «Даже если вы считаете более справедливым немедленно встроить _правильный_ результат о защите животных в будущее, так чтобы вашей экстраполированной воле не нужно было тратить часть своей силы на голосование за это, не все могут считать это справедливым. С нашей точки зрения, как программистов, у нас нет особой причины слушать вас вместо Алисы. Мы не обсуждаем, будут ли животные защищены, если незначительное веганское меньшинство очень захочет\* этого, а остальной части человечества будет все равно\*. Мы обсуждаем, должна ли именно ваша экстраполированная воля справедливо потратить некоторую часть своей переговорной силы, чтобы убедиться, что животные будут защищены, даже если у большинства нет такого желания\*. Как программистам нам это кажется вполне разумным, исходя из нашего желания быть честными, не быть козлами и не начинать битв за мировое господство.»

Этот третий ответ особенно важен, потому что первые два ответа, взятые по отдельности — «вы можете ошибаться насчет того, что это хорошая идея» и «даже если вы заботитесь об их благополучии, вам могут не понравиться их экстраполированные воли» — могут [точно также применяться][prove_too_much] для того, чтобы утверждать, что участники проекта КЭВ должны экстраполировать только свои собственные воли, а не воли остального человечества:

- Мы можем ошибаться, полагая, что это хорошая идея — экстраполировать воли всех остальных. Их включение в проект КЭВ сделает такое решение неизменным. Если мы правы насчет того, что должны запустить «КЭВ для всех», — если мы предсказуемо пришли бы к такому выводу после некоторых размышлений, — то наши экстраполированные воли и так могут сделать это за нас.
- Отказ экстраполировать воли других людей не подразумевает, что мы не должны заботиться о них. Мы можем быть правы насчет того, что должны заботиться о благополучии других, но в их экстраполированных волях могут быть заложены какие-то ужасные идеи.

Предлагаемый способ решения этого вопроса заключался в запуске составного КЭВ с проверкой от КЭВ участников и резервным КЭВ на случай провала проверки. Но тогда почему бы не запустить КЭВ всех животных с проверкой от КЭВ участников, прежде чем переходить к КЭВ ныне живущих людей?

Один из вариантов — вернуться к третьему ответу выше: нечеловеческие млекопитающие не участвуют в проекте КЭВ, не работают над его осуществлением и не станут злиться на людей, стремящихся захватить мир без видимой заботы о честности. Таким образом, они не входят в точку Шеллинга, где «каждый человек получает экстраполированный голос».

<h3 id="#10.2">Почему бы не включить всех разумных существ?</h3>

Точно также, можно спросить: «Почему бы не включить всех разумных существ, которых ИИ подозревает в существовании в мультивселенной, с учетом их распространенности в ней?»

- Потому что многие из них могут иметь столь же чуждые ЭВ, как и ЭВ осы-ихневмониды.
- Потому что наши ЭВ всегда могут сделать это, если это на самом деле хорошая идея.
- Потому что их нет здесь, чтобы возмутиться и отозвать свою политическую поддержку, если мы не включим их немедленно в основу для экстраполяции.

<h3 id="#10.3">Почему бы не включить умерших людей?</h3>

«Почему бы не включить всех умерших людей помимо всех ныне живущих?»

В этом случае нельзя ответить, что они не внесли вклад в человеческий проект (напр., Ирвинг Джон Гуд). Также их ЭВ вряд ли будет более чуждым, чем в любом другом из рассмотренных выше случаев.

Но мы снова возвращаемся к третьему ответу: «люди, которые всё ещё живы» — это простой круг Шеллинга, который включает всех, кто участвует в текущем политическом процессе. Если было бы правильно или честно экстраполировать Лео Силарда и включить его, мы и так сможем сделать это, если супербольшинство ЭВ решит\*, что это было бы правильно или честно. А если мы _не заложим_ это решение в модель, Лео Силард не восстанет из могилы, чтобы нас упрекнуть. Это кажется достаточной причиной рассматривать «людей, которые всё ещё живы» как простую и очевидную основу для экстраполяции.

<h3 id="#10.4">Зачем включать бессильных людей?</h3>

«Зачем включать в экстраполяцию очень маленьких детей, не контактировавшие ни с кем племена, которые никогда не слышали об ИИ, и пациентов, находящихся в криозаморозке (если такие есть)? Они в своём нынешнем состоянии не могут проголосовать ни за, ни против чего-либо.»

- Многое в интуитивной мотивации КЭВ сводится к тому, чтобы не быть козлами, а игнорирование желаний бессильных живых людей кажется интуитивно гораздо более стремным, чем игнорирование желаний бессильных мёртвых людей.
- Эти люди фактически будут присутствовать в будущем, так что кажется менее стремным учесть их желания при формировании этого будущего, чем не учесть.
- Если этого не сделать, могут быть обижены их родственники.
- Это сохраняет границу Шеллинга простой.

---

[^1]: Прим. пер.: Здесь и далее знак звездочки (\*), судя по всему, используется для того, чтобы подчеркнуть, что речь идет об экстраполированных версиях чего-либо.

[^2]: Хотя на практике вы бы не хотели, чтобы проект ИИ предпринял целую дюжину попыток определения КЭВ. Потому что это будет означать, что с методом, который используется для генерации предлагаемых вариантов, что-то очень не так. Какая бы попытка ни оказалась успешной после такого, скорее всего это будет первый ответ, в котором все его недостатки просто скрыты, а не первый, в котором они были устранены.

{% include routes.html %}
